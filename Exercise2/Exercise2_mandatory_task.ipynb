{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affective Computing - Programming Assignment 2\n",
    "\n",
    "### Objective\n",
    "\n",
    "Your task is to extract a set of prosodic correlates (i.e. suprasegmental speech parameters) and cepstral features from speech recordings. Then, an emotion recognition system is constructed to recognize happy versus sad emotional speech (a quite easy two class problem) using a simple supervised classifier training and testing structure.\n",
    "\n",
    "The original speech data is a set of simulated emotional speech (i.e. acted) from ten speakers speaking five different pre-segmented sentences of roughly 2-3 seconds in two different emotional states (happy and sad) totaling 100 samples.\n",
    "Basic prosodic features (i.e. distribution parameters derived from the prosodic correlates) are extracted using a simple voiced/unvoiced analysis of speech, pitch tracker, and energy analysis. Another set of Mel-Frequency Cepstral Coefficients (MFCC) features are also calculated for comparison. \n",
    "\n",
    "Support Vector Machine (SVM) classifiers are trained. A random subset of 1/2 of the available speech data (i.e. half of the persons) is used to train the emotion recognition system, first using a set of simple prosodic parameter features and a then a classical set of MFCC derived features. The rest of the data (the other half of the persons) is then used to evaluate the performances of the trained recognition systems.\n",
    "\n",
    "<!--### Implementation\n",
    "<!---The data and toolbox files used in this exercise are:\n",
    "Study the toolbox functions (e.g. ‘getF0’, ‘melcepst’) and the generic MATLAB functions (e.g. ‘hamming’, ‘conv’, ‘resample’, ‘filter’, ‘mean’, ‘std’, ‘prctile’, ‘kurtosis’, ‘sum’, ‘length’, ‘linspace’, ‘trainsvm’, ‘svmclassify’, and ‘confusionmat’) as they are needed in the exercise.-->\n",
    "\n",
    "<!--Nine dictionaries are stored in the provided data file:-->\n",
    "\n",
    "<!--* speech_sample\n",
    "* testing_class \n",
    "* testing_data_mfcc \n",
    "* testing_data_proso \n",
    "* testing_personID \n",
    "* training_class \n",
    "* training_data_mfcc \n",
    "* training_data_proso \n",
    "* training_personID -->\n",
    "\n",
    "<!--To access one dictionary, using [`scipy.io`](https://docs.scipy.org/doc/scipy/reference/io.html) library for example: scipy.io.loadmat('filePath')['dictoionaryName']. **speech_sample** is used in the feature extraction part and the pre-extracted features in the emotion recognition part of this lab are **testing_class**, **testing_data_mfcc**, **testing_personID**, **training_class**, **training_data_mfcc**, **training_data_proso**, **training_personID**.-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0. Preparation\n",
    "Downsample the ‘speech_sample’ from the original Fs of 48 kHz to 11.025 kHz using [`scipy.signal.resample()`](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.signal.resample.html) function.\n",
    "\n",
    "**Steps**:\n",
    "1. Load the data 'speech_sample' from file *lab2_data.mat*. Make sure the sample is a 1-D time series by reshaping it.\n",
    "2. Declare the sampling frequency of the original signal, and the new sampling frequency.\n",
    "3. Resample the signal using [`scipy.signal.resample()`](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.signal.resample.html).\n",
    "4. Visualize the resampled signal in the time domain. Use an appropriate time vector as the x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--### Task 0.1. Load Data\n",
    "Load the ‘speech_sample’ from the provided dataset containing a raw speech waveform and do the following (Note, the sampling rate (fs) of the sample speech signal is 48 kHz):-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load the 'speech_sample'\n",
    "exercise_data = sio.loadmat('lab2_data.mat')\n",
    "speech_sample = exercise_data['speech_sample'].reshape(-1)\n",
    "\n",
    "# 2. Declare the source sampling frequency, and the target sampling frequency.\n",
    "#    2.1 Source sampling frequency\n",
    "fs_source =\n",
    "\n",
    "#    2.2 Target sampling frequency\n",
    "# Target frequency\n",
    "fs_down =\n",
    "\n",
    "# 3. Downsample the speech sample\n",
    "speech_resampled = signal.resample(speech_sample, np.round(len(speech_sample) * fs_down / fs_source).astype('int'))\n",
    "\n",
    "# 4. Visualize the downsampled speech signal in the time domain.\n",
    "#    4.1 Create the corresponding time vector, whose length is the same as the length of the given signal. \n",
    "#        Use either np.linspace() or np.arange()\n",
    "\n",
    "\n",
    "#    4.2 Plot your result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 MFCC calculations using the provided sample speech signal.\n",
    "\n",
    "**Steps**:\n",
    "1. Pre-emphasize the resampled signal by applying a high pass filter, using the [`scipy.signal.lfilter()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.lfilter.html) function.\n",
    "   Apply a pre-emphasis filter $ H(z) = 1- \\alpha z^{-1} $ with $\\alpha = 0.98$ to emphasize higher frequencies in your downsampled speech signal (Tip: use [`scipy.signal.lfilter`](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.signal.lfilter.html)). \n",
    "   \n",
    "   Hint for defining the filter: you will provide two vectors **b** and **a** to define the filter, **a** for the denominator and **b** for the numerator. So finally your filter will be defined as $$H(z) = \\frac{b[0] z^0 + b[1] z^{-1} + ... + b[i] z^{-i}+...}{a[0] z^0 + a[1] z^{-1} + ... + a[i] z^{-i}+...}$$\n",
    "2. Extract the 12 mfcc coefficients by using the [`python_speech_features.mfcc()`](http://python-speech-features.readthedocs.io/en/latest/) function.\n",
    "    1. **The [`python_speech_features.mfcc()`](http://python-speech-features.readthedocs.io/en/latest/) function has an internal pre-emphasis functionality. However, we calculate the pre-emphasis by hand in order to have a better understanding of it, and thus it should be set to 0** \n",
    "3. Visualize the 12 mfcc coefficient contours.\n",
    "4. Calculate the mean of each contour using [`numpy.mean(axis=axis)`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import lfilter\n",
    "from python_speech_features import mfcc\n",
    "# 1. Pre-emphasize the resampled signal.\n",
    "#    1.1 Define the polynomials of the fitler\n",
    "#        filter coefficients b, which is the numerator\n",
    "#        filter coefficients a, which is the denominator\n",
    "a =\n",
    "b =\n",
    "\n",
    "#    1.2 Apply the filter on the signal\n",
    "pre_emphasized_sample = lfilter(..., ..., ...)\n",
    "\n",
    "# 2. Extract the mfcc coefficients by using the mfcc() function\n",
    "     # remeber to set the pre-emphasize argument to 0 since the signal has been pre-emphasized already.\n",
    "frame_len = int(2 ** np.floor(np.log2(0.03 * fs_down)))\n",
    "mfcc_contour = mfcc(pre_emphasized_sample, fs_down, winlen = frame_len / fs_down,\n",
    "                    winstep = frame_len / (2 * fs_down), numcep = 12, preemph = 0)\n",
    "\n",
    "# 3. Plot the 12 mfcc contours\n",
    "\n",
    "\n",
    "# 4. Calculate the mean for each contour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. Why do we need to pre-emphasize the speech signal before computing the MFCC feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Extract the Intensity/Energy parameter\n",
    "Firstly, calculate the short time energy (STE) of the downsampled ‘speech_sample’ using the squared signal $x(t)^2$ and a 0.01s hamming window frames (Note! the extra length of the window. Clip half a window length from the beginning and at the end). Then calculate the 5 distribution parameter features specified below from the utterance (the signal).\n",
    "\n",
    "\n",
    "**Steps**:\n",
    "1. Define a hamming window using the [`scipy.signal.hamming()`](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.signal.hamming.html) function. The window length is the number of frames in 0.01s.\n",
    "\n",
    "2. Apply the hamming window to convolve the squared signal, using the [`scipy.signal.convolve()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve.html) function. The convolution result is the short time energy (STE) controu.\n",
    "3. Clip half window of frames from the begining and ending of STE contour.\n",
    "4. Visualize the resulted STE controur.\n",
    "5. Calculating the following 5 distribution parameter feature from the STE contour:\n",
    "    1. Mean, using the [`numpy.mean(https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.mean.html)`]() function.\n",
    "    2. Standard Deviation (SD), using the [`numpy.std()`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.std.html) function.\n",
    "    3. 10% percentile, using the [`numpy.percentile()`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.percentile.html) function.\n",
    "    4. 90% percentile, using the [`numpy.percentile()`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.percentile.html) function.\n",
    "    5. Kurtosis, using the [`scipy.stats.kurtosis()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kurtosis.html) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "# 1. Define a hamming window\n",
    "#    1.1 Calculate the window length, which is the number of frames within 0.01s\n",
    "hamming_length =\n",
    "\n",
    "#    1.2 Define the hamming window using signal.hamming()\n",
    "hamming_window = signal.hamming(hamming_length)\n",
    "\n",
    "# 2. Calculate the short time energy (STE) contour by convolving the hamming window and the squared signal, \n",
    "#    using the scipy.signal.convolve() function\n",
    "ste = \n",
    "\n",
    "# 3. Clip half a window of frames from both the beginning and end of the STE contour\n",
    "\n",
    "\n",
    "# 4. Visualize the final STE contour.\n",
    "#    4.1 Create the time vector for x-axis\n",
    "\n",
    "#    4.2 Visualize the STE contour\n",
    "\n",
    "\n",
    "# 5. Calculate the 5 distribution parameter features the of STE contour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. Why do we need to clip out half a frame from both the beginning and the ending of the STE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3. Extract the Pitch/F0 feature\n",
    "**Steps**:\n",
    "1. Extract the Pitch/F0 contour of the resampled speech signal using the **get_f0()** function in 0.01s frames. The function is provided in the *f0_lib.py* file.\n",
    "2. Visualize the F0 contour.\n",
    "3. Extract the 5 distribution parameter features of the extracted F0 countour.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from f0_lib import get_f0\n",
    "\n",
    "# 1. Extract the F0 contour\n",
    "f0, _, _, _ = get_f0(..., ...)\n",
    "\n",
    "# 2. Visualize the F0 contour\n",
    "\n",
    "\n",
    "# 3. Calculate these distribution parameter features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4. Extract the Rhythm/Durations parameter\n",
    "**Steps**:\n",
    "1. Perform a Voiced/Unvoiced speech segmentation of speech signal. Tip: Unvoiced frames are marked with 0 F0 values, you can find the voiced frames (i.e. F0 > 0) using [`numpy.where()`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.where.html).\n",
    "2. From the segmentation, calculate the means and SDs of both Voiced and Unvoiced segment lengths (i.e. voiced segment mean length, SD of voiced segment lengths, unvoiced segment mean length, SD of unvoiced segment lengths).\n",
    "3. Calculate also the voicing ratio, i.e. the ratio of voiced segments versus total segments (Tip: You can do this simply by using the frames).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Segmenting the voiced and unvoiced speech segements.\n",
    "voiced_inds = np.where(f0 > 0)[0]\n",
    "diff = voiced_inds[1:] - voiced_inds[: -1]\n",
    "voiced_to_unvoiced_inds = np.where(diff > 1)[0]\n",
    "voiced_seg_n = len(voiced_to_unvoiced_inds) + 1\n",
    "voiced_seg_lengths = np.zeros(voiced_seg_n)\n",
    "tmp = voiced_inds[0]\n",
    "\n",
    "for i in range(voiced_seg_n - 1):\n",
    "    voiced_seg_lengths[i] = voiced_inds[voiced_to_unvoiced_inds[i]] - tmp + 1\n",
    "    tmp = voiced_inds[voiced_to_unvoiced_inds[i] + 1]\n",
    "    \n",
    "voiced_seg_lengths[-1] = voiced_inds[-1] - tmp + 1\n",
    "\n",
    "######################################################\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Calculate the means and SDs of both Voiced and Unvoiced segment lengths\n",
    "\n",
    "\n",
    "# 3. Calculate the voicing ratio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. What is the difference between voiced and unvoiced sounds? Give some examples of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5. Check the extracted features\n",
    "1. Print your calculated 12 MFCC coefficients.\n",
    "2. Print the distribution parameter features of the STE contour.\n",
    "3. Print the distribution parameter features of the F0 contour.\n",
    "4. Print the 5 prodosic features: **mean** and **std** of lengths of **voiced speeches** and **unvoiced speech**, as well as the **voicing ratio**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print the 12 MFCC coefficients\n",
    "\n",
    "# 2. Print the distribution paremeter features of the STE contour\n",
    "\n",
    "# 3. Print the distribution parameter features of the F0 contour\n",
    "\n",
    "# 3. Print the 5 prodosic features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2. Speech Emotion Classification\n",
    "\n",
    "In this part, the [`sklearn.svm`](http://scikit-learn.org/stable/modules/svm.html) library is used to perform the speech signal classification. The **‘training_data_proso’** and **‘training_data_mfcc’** matrices contain the calculated prosodic features for the training set (9 features in each row representing a speech sample) and MFCC derived features (12 features) respectively. The **‘training_class’** group vector contains the class of samples: 1 = happy, 2 = sad; corresponding to the rows of the training data matrices.\n",
    "\n",
    "<!---\n",
    "Test the classifiers and plot confusion matrices.\n",
    "* Use the ‘svmclassify’ function (and your trained SVM structures) to classify the ‘training_data_*’ and the ‘testing_data_*’ data matrices. Then, calculate average classification performances for both training and testing data. The correct class labels corresponding with the rows of the training and testing data matrices are in the variables ‘training_class’ and ‘testing_class’, respectively.\n",
    "    * \tCalculate the average classification performances for the training data (‘training_data_proso’ and ‘training_data_mfcc’) using the corresponding prosody and MFCC trained SVMs.\n",
    "    * \tCalculate the average classification performance for the testing data (‘testing_data_proso’ and testing_data_mfcc’) using the corresponding prosody and MFCC trained SVMs.\n",
    "* Plot confusion matrices for the training and testing data for both classifiers. Tip, use ‘confusionmat’ function.-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---speech_sample\n",
    "testing_class\n",
    "testing_data_mfcc\n",
    "testing_data_proso\n",
    "testing_personID\n",
    "training_class\n",
    "training_data_mfcc\n",
    "training_data_proso\n",
    "training_personID\n",
    "### Task 2.1. Preparing your data\n",
    "Dictionaries of the data are listed below:\n",
    "* speech_sample\n",
    "* testing_class\n",
    "* testing_data_mfcc\n",
    "* testing_data_proso\n",
    "* testing_personID\n",
    "* training_class\n",
    "* training_data_mfcc\n",
    "* training_data_proso\n",
    "* training_personID\n",
    "Use [`scipy.io.loadmat()`] to read the dataset.-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1. Train the SVM classifiers\n",
    "**Steps**:\n",
    "1. Load the training data.\n",
    "2. Train a SVM with the prosody data using the **‘training_data_proso’** features and a **3rd order polynomial** kernel.\n",
    "3. Train a SVM with the MFCC data using the **‘training_data_mfcc’** features and a **3rd order polynomial** kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SVM classifiers.\n",
    "from sklearn import svm\n",
    "\n",
    "# 1. Load data\n",
    "exercise_data = sio.loadmat('lab2_data.mat')\n",
    "\n",
    "#    1.1 Load 'training_data_proso'\n",
    "training_data_proso = exercise_data['training_data_proso']\n",
    "\n",
    "#    1.2 Load 'training_data_mfcc'\n",
    "training_data_mfcc = exercise_data['training_data_mfcc']\n",
    "\n",
    "\n",
    "#    1.3 Load 'training_class'\n",
    "training_class = exercise_data['training_class'].reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Train a classifier using the prodosic data\n",
    "#    2.1 Initialize a svm classifer\n",
    "\n",
    "\n",
    "#    2.2 Train the classifier\n",
    "\n",
    "\n",
    "# 3. Train a classifer using the mfcc data\n",
    "#    2.1 Initialize a svm classifer\n",
    "\n",
    "\n",
    "\n",
    "#    2.2 Train the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2. Test the classifiers\n",
    "Classify the **‘training_data_*’** and the **‘testing_data_*’** data matrices. Then, calculate average classification performances for both training and testing data. The correct class labels corresponding with the rows of the training and testing data matrices are in the variables ‘training_class’ and ‘testing_class’, respectively.\n",
    "\n",
    "**Steps**:\n",
    "1. Load the testing data.\n",
    "2. Calculate the average classification accuracy for the training data (**‘training_data_proso’** and **‘training_data_mfcc’**) using the corresponding prosody and MFCC trained SVMs.\n",
    "3. Calculate the average classification accuracy for the testing data (**‘testing_data_proso’** and **‘testing_data_mfcc’**) using the corresponding prosody and MFCC trained SVMs.\n",
    "4. Print the four accuracies you have calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load testing data\n",
    "testing_data_mfcc = exercise_data['testing_data_mfcc']\n",
    "testing_data_proso = exercise_data['testing_data_proso']\n",
    "testing_class = exercise_data['testing_class'].reshape(-1)\n",
    "\n",
    "# 2. Calculate the average classification performances for the training data\n",
    "\n",
    "\n",
    "# 3. Calculate the average classification performance for the testing data\n",
    "\n",
    "\n",
    "\n",
    "# 4. Print the four accuracies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4. Which one of the extracted features performed better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3. Plot confusion matrices for the training and testing data for both classifiers. \n",
    "Print following confusion matrix(Tip, use [`sklearn.metrics.confusion_matrix`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function):\n",
    "* The confusion matrix of the prosody trained SVM using the **‘training_data_proso’**.\n",
    "* The confusion matrix of the prosody trained SVM using the **‘testing_data_proso’**.\n",
    "* The confusion matrix of the MFCC trained SVM using the **‘training_data_mfcc’**.\n",
    "* The confusion matrix of the MFCC trained SVM using the **‘testing_data_mfcc’**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Speech Emotion Classification, Subject Independent\n",
    "Generate a person independent 10-fold cross-validation (CV) estimate of the emotion recognition system performance.\n",
    "* Join the training/testing data matrices and the class vectors. Combine also the ‘training_data_personID’ and ‘testing_data_personID’ vectors that are needed to make the CV folds.\n",
    "\n",
    "* Construct the CV folds by training ten SVMs. For each SVM nine persons’ data is used as the training set (i.e. 90 samples) and one persons’ samples are kept as the test set (i.e. 10 samples) for the respective fold (i.e. each SVM has different persons’ samples excluded from the training set). Test each ten trained SVMs by using the corresponding one held-out persons’ samples and then calculate the average classification performances for each fold.\n",
    "\n",
    "* Calculate the mean and SD of the ten CV fold performances to produce the final CV performance estimate of the emotion recognition system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
