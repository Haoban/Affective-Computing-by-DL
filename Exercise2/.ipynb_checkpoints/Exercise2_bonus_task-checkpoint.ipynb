{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (Bonus Task). Emotion Speech Recognition using Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)\n",
    "\n",
    "### Objective\n",
    "\n",
    "**This exercise task asks you to conduct deep learning based emotion speech recognition using the given model architecture on the given data**.\n",
    "Generally speaking, you are asked to predict the emotion of the speaker based on given speech waveforms with arbitrary lengths. \n",
    "In this task, you need to use the Convolutional Neural Network (CNN) which we have used in the bonus task of Exercise 1, but with 1-dimension filers. \n",
    "Besides, you also need to use a sequential data analysis deep learning module, the [Long Short-Term Memory (LSTM)](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n",
    "The 1-d CNN here is used to extract locally short time feature from speech frames, \n",
    "and the LSTM is used to synthesize the globally long-term feature based on these short time features. \n",
    "The network architecture will be end-to-end, which receives the sequence of waveform frames as the inputs and directly give the prediction of the emotion. As a result, the philosophy of 'short time analysis' is still applied here and you need to make the speech waveform as a sequence of equal-length waveform frames and feed them to the network.\n",
    "\n",
    "In this part, the dataset to be used is a subset of [*Toronto emotional speech set (TESS)*](https://tspace.library.utoronto.ca/handle/1807/24487). The TESS dataset contains the utterance which are spoken under 7 categories of emotions by two actresses (young actress and old actress). In this exercise, we select two categories of data (happy and sad) which is generated by young speakers. In total there are 400 samples (200 samples for each class). The **training and evaluation protocol will be that 70% of the data will be selected as the training data, and the rest will be the test data**. \n",
    "\n",
    "\n",
    "Similar to the bonus task of the exercise 1, we have provided the code which defines the network architecture using pytorch, and you will need to invoke it in your training and evaluation code. You are also encouraged to implement your own network, and all software framework are accepted. The network architecture is described in the figure below.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested procedures\n",
    "\n",
    "We provide following procedures to support you to complete this exercise. But you are free to achieve the exercise goal by your own way of implementation.\n",
    "\n",
    "1. Load and normalize the speech data by subtract the mean and divide the stand variance which are calculated from the whole dataset. To load the speech data, you can use the [librosa.load()](https://librosa.github.io/librosa/generated/librosa.core.load.html#librosa-core-load) for example, and you can also use other tools if you like.\n",
    "\n",
    "2. Segment frames from the speech waveforms in order to perform short-time analysis. For example you can make your window size as 20-40 ms, with or without overlapping.\n",
    "\n",
    "3. Split the dataset to training set and testing set, the testing set will be 30% of the whole dataset.\n",
    "\n",
    "3. Initialize the network and perform the batch training\n",
    "\n",
    "4. Evaluated the trained mode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code snippet of the network architecture\n",
    "\n",
    "Below we provide the network architecture definition written in pytorch. Please take read them and use them in your further experiments.\n",
    "\n",
    "#### Code snippet of the LSTM cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LstmCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, inDim, outDim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.outDim = outDim\n",
    "        self.inDim = inDim\n",
    "        \n",
    "        \n",
    "        # Define the gate operation of LSTM, each gate is implemented by a fully connected neural network\n",
    "        # The input dimension of each gate is the sum of the dimension of input and out put\n",
    "        # because each gate performs the operations on input and previous output at the same time\n",
    "        \n",
    "        # For example at time step t, the input is x_t and h_{t-1}\n",
    "        \n",
    "        # For the example of input gate: I_t = Wx * x_t + Wh * h_{t-1} + b\n",
    "        # So we can use a one-time linear operation to complete it: make it as [x_t h_{t-1}] * [Wx;  Wh] + b\n",
    "        \n",
    "        self._i = nn.Linear(inDim + outDim, outDim)\n",
    "        self._f = nn.Linear(inDim + outDim, outDim)\n",
    "        self._o = nn.Linear(inDim + outDim, outDim)\n",
    "        self._g = nn.Linear(inDim + outDim, outDim)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, currentStates):\n",
    "        \n",
    "        \n",
    "        # Receive the cell state and output of prevous time step\n",
    "        currentH, currentC = currentStates\n",
    "        \n",
    "        # Concate the previous output and current input for the prepration of the gate operation\n",
    "        combined = torch.cat([x, currentH], dim=1)\n",
    "        \n",
    "        # Perform the gate operations\n",
    "        cc_i = self._i(combined)\n",
    "        cc_f = self._f(combined)\n",
    "        cc_o = self._o(combined)\n",
    "        cc_g = self._g(combined)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        \n",
    "        \n",
    "        # Update the cell state\n",
    "        nextC = f * currentC + i * g\n",
    "        \n",
    "        # Calculate the output\n",
    "        nextH = o * torch.tanh(nextC)\n",
    "        \n",
    "        \n",
    "        return nextH, nextC\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batchSize):\n",
    "        \n",
    "        # Initialize the variables, cell states and output states at t=0\n",
    "        return (Variable(torch.zeros(batchSize, self.outDim)).double(),\n",
    "                Variable(torch.zeros(batchSize, self.outDim)).double())\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code snippet of the LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LstmLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, inDim, outDim):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.lstmCell = LstmCell(inDim=inDim, outDim=outDim)\n",
    "    \n",
    "    def forward(self, x, sequenceLengths, returnLast=False, hc=None):\n",
    "        N, T, D = x.shape\n",
    "        \n",
    "        # A LSTM layer handles he operations from t=0 till T.\n",
    "        # So the cell state has to be initialized as zerso\n",
    "        if hc is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            hc = self._init_hidden(batchSize=N)\n",
    "            \n",
    "        h, c = hc\n",
    "        \n",
    "        outH = []\n",
    "        outC = []\n",
    "\n",
    "        # Iterate for each timestep.\n",
    "        for t in range(T):\n",
    "            h, c = self.lstmCell(x=x[:, t, :], currentStates=[h,c])\n",
    "            \n",
    "            outH.append(h)\n",
    "            outC.append(c)\n",
    "            \n",
    "        outH = torch.stack(outH, dim=1)\n",
    "        outC = torch.stack(outC, dim=1)\n",
    "        \n",
    "        if returnLast:\n",
    "            newOutH = []\n",
    "            newOutC = []\n",
    "            \n",
    "            # Since the the length of input are different, and some of them are padded by zeros,\n",
    "            # so the last output has to be picked according to the length of the input sequence\n",
    "            for sampleIte in range(N):\n",
    "                newOutH.append(outH[sampleIte, sequenceLengths[sampleIte] - 1, :])\n",
    "                               \n",
    "                #sampleIte, outH[sequenceLengths[sampleIte] - 1, :])\n",
    "                newOutC.append(outC[sampleIte, sequenceLengths[sampleIte] - 1, :])\n",
    "                \n",
    "            newOutH = torch.stack(newOutH, dim=0)\n",
    "            newOutC = torch.stack(newOutC, dim=0)\n",
    "            return newOutH, newOutC    \n",
    "            \n",
    "        else:\n",
    "            return outH, outC\n",
    "    \n",
    "    def _init_hidden(self, batchSize):\n",
    "        return self.lstmCell.init_hidden(batchSize=batchSize)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code snippet of the whole nework architecture\n",
    "In oder to use the provide network architecture, please read the comment of forward() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioNet(nn.Module):\n",
    "\n",
    "    def __init__(self, numClass=2, inDim=0, convLayerNum=3, convOutNum=64, convKernelSize=5, lstmLayerNum=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convLayerNum = convLayerNum\n",
    "        self.convOutNum = convOutNum\n",
    "        self.lstmLayerNum=lstmLayerNum\n",
    "        self.convKernelSize = convKernelSize\n",
    "        self.inDim = inDim\n",
    "        \n",
    "        \n",
    "        self.currentDim = self.inDim\n",
    "        self.avgPool = nn.AvgPool1d(kernel_size=2,\n",
    "                                    stride=2)\n",
    "        \n",
    "        self.currentDim = self.currentDim / 2\n",
    "        self.conv1d_1 = nn.Conv1d(in_channels=1,\n",
    "                                  out_channels=64,\n",
    "                                  kernel_size=self.convKernelSize,\n",
    "                                  stride=2)\n",
    "        \n",
    "        # (self.inDim/2 + 2 x 0 - 1 x (5 - 1) - 1) / 2 + 1\n",
    "        # (488 - 4)/2 + 1  \n",
    "        self.currentDim = int((self.currentDim + 2 * 0 - 1 * (self.convKernelSize - 1) - 1)/2 + 1) \n",
    "        \n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.currentDim = int(self.currentDim / 2)\n",
    "        \n",
    "        self.conv1d_2 = nn.Conv1d(in_channels=64,\n",
    "                                  out_channels=64,\n",
    "                                  kernel_size=self.convKernelSize,\n",
    "                                  stride=2)\n",
    "        for i in range(self.convLayerNum - 1):\n",
    "            self.currentDim = int((self.currentDim + 2 * 0 - 1 * (self.convKernelSize - 1) - 1)/2 + 1) \n",
    "            self.currentDim = int(self.currentDim/2)\n",
    "        \n",
    "        self.currentDim = convOutNum * self.currentDim\n",
    "        self.lstm1 = LstmLayer(self.currentDim, 64)\n",
    "        self.lstm2 = LstmLayer(64, 64)\n",
    "        self.fc = nn.Linear(convOutNum, numClass)\n",
    "                    \n",
    "    def forward(self, x, sequenceLengths):\n",
    "        '''\n",
    "        input size:\n",
    "            xï¼š N x T x D\n",
    "            sequence: N x 1\n",
    "            \n",
    "            \n",
    "        Example of input:\n",
    "            If you have three sequence of speech frames with different lengths:\n",
    "                Frame11 Frame12 Frame 13    0     \n",
    "                Frame21 Frame22 Frame 23 Frame 24\n",
    "                Frame31 frame32    0        0\n",
    "            \n",
    "            Then you need to pad the sequence with empty frames to make them have eaual length in time, and put them in a tensor,\n",
    "            as a result the tensor will have the size of 3 x 4 x D\n",
    "            \n",
    "            Besides, you also need to provide a vector which contains their lengths, in this example it will be:\n",
    "                [3\n",
    "                 4\n",
    "                 2]\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        N, T, D = x.shape\n",
    "        x = torch.reshape(x, (N * T, 1,  D))\n",
    "        \n",
    "        # Applying an initial downsampling\n",
    "        x1 = self.avgPool(x)\n",
    "        # Applying an intial convolution\n",
    "        x2 = self.conv1d_1(x1)    \n",
    "        x2 = F.relu(x2)\n",
    "        x3 = self.maxPool(x2)\n",
    "        \n",
    "\n",
    "        # Apply several layers of CNN\n",
    "        for convLayerIte in range(self.convLayerNum - 1):\n",
    "            \n",
    "            x3 = self.conv1d_2(x3)\n",
    "            x3 = F.relu(x3)\n",
    "            x3 = self.maxPool(x3)\n",
    "        \n",
    "        x4 = torch.reshape(x3, (N, T, -1))\n",
    "        \n",
    "        # Apply two LSTM layers\n",
    "        x4, _ = self.lstm1(x=x4, sequenceLengths=sequenceLengths, returnLast=False, hc=None)\n",
    "        x5, _ = self.lstm2(x=x4, sequenceLengths=sequenceLengths, returnLast=True, hc=None)\n",
    "                \n",
    "        \n",
    "        x6 = F.relu(x5)\n",
    "        return self.fc(x6)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your implementation\n",
    "Please write your code below to complete the exercise\n",
    "\n",
    "\n",
    "#### Load and normalize the speech data\n",
    "\n",
    "1. Get the sampling rate of the raw data. All waveforms are recorded with the same sampling rate, so you can get the sampling rate by only read on speech samples. The function [librosa.load()](https://librosa.github.io/librosa/generated/librosa.core.load.html#librosa-core-load) will return the sampling rate, please read the manual.\n",
    "\n",
    "2. Normalize the speech data by subtracting the *mean* and dividing the *stand variance* to make the data as 'zero mean and unit variance'. Thus you need to calculate the *mean* and *stand variance* from the whole dataset. Since speech waveforms are 1-d time series, so the *mean* and *stand variance* will be scalars. \n",
    "\n",
    "3. Segment each speech waveform as sequences of equal-length frames. You can make frames with or without overlapping according to your needs. Please not that you may need to pad zeros to the end of the speech waveforms in order to make the last frame has the same length with others.\n",
    "\n",
    "4. Split the dataset to training set and testing set. 70% of the data will be the training data, an 30% will be the testing data. The portion will be applied equally to either class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# filename = 'BonusTaskData/YAF_happy/YAF_back_happy.wav'\n",
    "# # filename = librosa.util.example_audio_file()\n",
    "\n",
    "# # To preserve the native sampling rate of the file, use sr=None.\n",
    "# y, sr = librosa.load(filename, sr=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import math\n",
    "\n",
    "#############################      Define device       ################################# \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "#############################      Load data       #################################\n",
    "\n",
    "data_dir = 'BonusTaskData'\n",
    "dataset = []\n",
    "original = []\n",
    "\n",
    "Videos = os.listdir(data_dir)\n",
    "maxlen = 50000\n",
    "\n",
    "for folder in Videos:\n",
    "    folderlist = os.listdir(data_dir+\"/\"+folder)\n",
    "    for video in folderlist:\n",
    "        if not os.path.isdir(video):\n",
    "            video_path = data_dir+\"/\"+folder+\"/\"+video\n",
    "            y, sr = librosa.load(video_path, sr=None)\n",
    "            y = y.tolist()\n",
    "            if len(y) > maxlen:\n",
    "                maxlen = len(y)\n",
    "            original.append(y)\n",
    "            dataset += y\n",
    "            \n",
    "sample_size = len(folderlist)            \n",
    "mean = np.mean(dataset)\n",
    "std = np.std(dataset, ddof = 1)\n",
    "classes_number = len(Videos)\n",
    "\n",
    "#############################     Normalize #################################\n",
    "\n",
    "\n",
    "Normalize = []\n",
    "\n",
    "for value in original:\n",
    "        Normalize.append([ (x-mean)/std  for x in value])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SegmentList = []\n",
    "SequenceLengths = []\n",
    "\n",
    "\n",
    "#############################        Segment        #################################\n",
    "# We set the window size is around 20 ms, Calculating the size the frame windows\n",
    "frame_size = (int(sr * 20 /1000/100)+1)*100\n",
    "\n",
    "Dimention = maxlen//frame_size+1\n",
    "\n",
    "for series in Normalize:\n",
    "    lent = len(series)//frame_size+1\n",
    "    SequenceLengths.append(lent)\n",
    "    frame = []\n",
    "    for k in range(lent-1):\n",
    "        frame.append(series[k*frame_size:(k+1)*frame_size])\n",
    "    frame.append(series[k*frame_size+len(series)%frame_size:])\n",
    "    for i in range(Dimention - lent):\n",
    "        frame.append([0.0]*frame_size)\n",
    "    SegmentList.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################     Split the dataset to training set and testing set      #################################\n",
    "import random\n",
    "\n",
    "classes1 = [0]*sample_size\n",
    "classes2 = [1]*sample_size\n",
    "classes = classes1 + classes2\n",
    "\n",
    "split_size = 0.7\n",
    "all_index = np.arange(sample_size*classes_number)\n",
    "train_index = np.vstack((random.sample(range(0,sample_size), int(sample_size * split_size)),random.sample(range(sample_size,sample_size*classes_number), int(sample_size * split_size)))).flatten()\n",
    "test_index = list(set(all_index)-set(train_index))\n",
    "\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "train_label = []\n",
    "test_label = []\n",
    "\n",
    "for index in train_index:\n",
    "    train_data.append(SegmentList[index])\n",
    "    train_label.append(classes[index])\n",
    "\n",
    "for index in test_index:\n",
    "    test_data.append(SegmentList[index])\n",
    "    test_label.append(classes[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data into Tensor(double)\n",
    "\n",
    "inputs_train_dataset = torch.from_numpy(np.array(train_data))\n",
    "labels_train_dataset = torch.LongTensor(np.array(train_label))\n",
    "\n",
    "inputs_test_dataset = torch.from_numpy(np.array(test_data))\n",
    "labels_test_dataset = torch.LongTensor(np.array(test_label))\n",
    "length_dataset = torch.from_numpy(np.array(SequenceLengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the network training.\n",
    "\n",
    "Please write your code below for the network training. You are asked to perform the batch based training here. Please accumulate the loss and classification accuracy for each epoch and output them and the end of the epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the network and optimizer\n",
    "model = AudioNet(numClass=classes_number,inDim=frame_size).double()\n",
    "\n",
    "#####Change model to cuda\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "trainingEpoch = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20Train loss: 0.699Train accuracy: 50.000\n",
      "Epoch 2/20Train loss: 0.697Train accuracy: 50.000\n",
      "Epoch 3/20Train loss: 0.697Train accuracy: 50.000\n",
      "Epoch 4/20Train loss: 0.697Train accuracy: 50.000\n",
      "Epoch 5/20Train loss: 0.697Train accuracy: 50.000\n",
      "Epoch 6/20Train loss: 0.697Train accuracy: 50.000\n",
      "Epoch 7/20Train loss: 0.697Train accuracy: 50.000\n",
      "Epoch 8/20Train loss: 0.696Train accuracy: 50.000\n",
      "Epoch 9/20Train loss: 0.696Train accuracy: 50.000\n",
      "Epoch 10/20Train loss: 0.695Train accuracy: 50.000\n",
      "Epoch 11/20Train loss: 0.695Train accuracy: 50.000\n",
      "Epoch 12/20Train loss: 0.693Train accuracy: 50.000\n",
      "Epoch 13/20Train loss: 0.691Train accuracy: 50.000\n",
      "Epoch 14/20Train loss: 0.687Train accuracy: 50.357\n",
      "Epoch 15/20Train loss: 0.789Train accuracy: 22.500\n",
      "Epoch 16/20Train loss: 0.673Train accuracy: 60.714\n",
      "Epoch 17/20Train loss: 0.637Train accuracy: 59.286\n",
      "Epoch 18/20Train loss: 0.554Train accuracy: 84.643\n",
      "Epoch 19/20Train loss: 0.481Train accuracy: 88.214\n",
      "Epoch 20/20Train loss: 0.453Train accuracy: 85.000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "batch_loss = 0\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for epoch in range(trainingEpoch):\n",
    "    train_len = 0\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    max_index = int(sample_size * split_size* classes_number)\n",
    "    for i in range(math.ceil(max_index/batch_size)):\n",
    "        start_index = i*batch_size\n",
    "        end_index = min((i+1)*batch_size,max_index)\n",
    "        inputs = inputs_train_dataset[start_index:end_index]\n",
    "        labels = labels_train_dataset[start_index:end_index]\n",
    "        inputs, labels = inputs.to(device),labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs= model.forward(inputs,length_dataset[start_index:end_index])\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "#    \n",
    "#    ps = torch.exp(outputs)\n",
    "#    top_p, top_class = ps.topk(1, dim = 1)\n",
    "#    print(top_p)\n",
    "#    equals = 1 if (top_class == labels.view(*top_class.shape)) else 0\n",
    "#    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "#    \n",
    "        # print training lost and accuracy\n",
    "        model.eval()\n",
    "        # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        train_len += labels.size(0)\n",
    "    print(f\"Epoch {epoch+1}/{trainingEpoch}\"\n",
    "      f\"Train loss: {running_loss/math.ceil(max_index/batch_size):.3f}\"\n",
    "      f\"Train accuracy: {100*running_corrects.double()/train_len:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conduct the network evaluation\n",
    "\n",
    "Please write your code below to evaluated the trained model using your splited testing data. Please output the loss and accuracy of the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.411Test accuracy: 89.167\n"
     ]
    }
   ],
   "source": [
    "# Test the network and print the testing result\n",
    "\n",
    "test_loss = 0\n",
    "test_corrects = 0\n",
    "test_len =0\n",
    "max_index2 = int(sample_size * (1-split_size)* classes_number)\n",
    "\n",
    "for i in range(math.ceil(max_index2/batch_size)):\n",
    "    start_index = i*batch_size\n",
    "    end_index = min((i+1)*batch_size,max_index2)\n",
    "    \n",
    "    inputs = inputs_test_dataset[start_index:end_index]\n",
    "    labels = labels_test_dataset[start_index:end_index]\n",
    "    \n",
    "    inputs, labels = inputs.to(device),labels.to(device)\n",
    "\n",
    "    outputs= model.forward(inputs,length_dataset[start_index:end_index])\n",
    "    \n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    batch_loss = criterion(outputs, labels)\n",
    "    test_loss += batch_loss.item()\n",
    "    test_corrects += torch.sum(preds == labels)\n",
    "    test_len += labels.size(0)\n",
    "    test_losses.append(test_loss/math.ceil(max_index2/batch_size))\n",
    "    \n",
    "    \n",
    "print(f\"Test loss: {test_loss/math.ceil(max_index2/batch_size):.3f}\"\n",
    "    f\"Test accuracy: {100*test_corrects.double()/test_len:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
