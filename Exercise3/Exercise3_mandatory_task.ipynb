{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affective Computing - Programming Assignment 3\n",
    "\n",
    "### Objective\n",
    "\n",
    "Your task is to use the feature-level method to combine the facial expression features and audio features. A multi-modal emotion recognition system is constructed to recognize happy versus sadness facial expressions (binary-class problem) by using a classifier training and testing structure. Furthermore, using decision-level method to combine the result of ECG signals and videos including facial expression to decide the pain result. \n",
    "\n",
    "For the the original data of feature-level method is based on lab1 and lab2, from ten actors acting happy and sadness behaviors. \n",
    "The original data of decision-level method is from a pain dataset including ECG signals and vidoes of facial expressions. \n",
    "* Task 1: Subspace-based feature fusion method: In this case, z-score normalization is utilized. Please read “Fusing Gabor and LBP feature sets for kernel-based face recognition” and learn how to use subspace-based feature fusion method for multi-modal system.\n",
    "\n",
    "* Task 2: Based on Task1, use Canonical Correlation Analysis to calculate the correlation coefficient of facial expression and audio features. Finally, use CCA to build a multi-modal emotion recognition system. The method is described in one conference paper “Feature fusion method based on canonical correlation analysis and handwritten character recognition”\n",
    "\n",
    "* Task 3: Decision fusion based method, use ECG signals and vidoes of facial expressions to recognize the pain result. Finally, combine the decision of ECG signals and vidoes of facial expressions to build a multi-modal recognition system. \n",
    "\n",
    "* Task 4: Use feature-level method (Task 2) on 10-fold cross-validation estimate of the emotion recognition system performance\n",
    "\n",
    "To produce emotion recognition case, Support Vector Machine (SVM) classifiers are trained.  50 videos from 5 participants are used to train the emotion recognition, use spatiotemporal features. The rest of the data (50 videos) is used to evaluate the performances of the trained recognition systems.\n",
    "\n",
    "To produce the pain recognition result case in task 3, Support Vector Machine (SVM) classifiers are trained. 40 samples are used to train the pain recongtion model. And 30 samples are used to evaluate the performances of the trained recognition systems. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Subspace-based method\n",
    "Please read “Fusing Gabor and LBP feature sets for kernel-based face recognition” and apply their framework for the exercise. We use Support Vector Machine (SVM) with linear kernel for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment \n",
    "\n",
    "First, we need to import the basic modules for loading the data and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import dlib\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "from skimage import color\n",
    "from skimage import img_as_ubyte\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import sklearn\n",
    "import scipy.io as sio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data \n",
    "\n",
    "We load the facial expression data (training data, training class, testing data, testing class) and audio data (training data, testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata = sio.loadmat('lab3_data.mat')\n",
    "\n",
    "#facial expression training and testing data, training and testing class\n",
    "training_data = \n",
    "testing_data = \n",
    "training_class = \n",
    "testing_class = \n",
    "\n",
    "#audio training and testing data\n",
    "training_data_proso = \n",
    "testing_data_proso = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the subspace for facial expression feature and audio features. \n",
    "Extract the subspace for facial expression feature and audio features using principal component analysis through using __[`sklearn.decomposition.PCA()`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)__ function.\n",
    "ReducedDim is the dimensionality of the reduced subspace.\n",
    "Set ReducedDim to 20 and 15 for facial expression feature and audio feature, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "\n",
    "#set ReducedDim for facial expression feature and audio feature, respectively.\n",
    "reducedDim_v = 20;\n",
    "reducedDim_a = 15;\n",
    "\n",
    "#Extract subspace for facial expression feature though PCA\n",
    "#set n_components\n",
    "pca_v=PCA()\n",
    "pca_v.fit()\n",
    "#Transform training_data and testing data respectively\n",
    "[] =pca_v.transform()\n",
    "[] =pca_v.transform()\n",
    "\n",
    "#Extract subspace for audio features though PCA\n",
    "pca_a=PCA()\n",
    "pca_a.fit()\n",
    "#Transform training_data and testing data respectively\n",
    "[]  =pca_a.transform()\n",
    "[]  =pca_a.transform()  \n",
    "\n",
    "\n",
    "#Concatenate ‘video training_data’ and ‘audio training_data’ into a new feature ‘combined_trainingData’\n",
    "sample_train = np.concatenate()  \n",
    "\n",
    "#Concatenate ‘video testing_data’ and ‘audio testing_data2 into a new feature ‘combined_testingData’.\n",
    "sample_test = np.concatenate()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature classification\n",
    "Use the __[`SVM`](http://scikit-learn.org/stable/modules/svm.html)__ function to train Support Vector Machine (SVM) classifiers.\n",
    "Construct an SVM using the ‘combined_trainingData’ and linear kernel. The ‘training_class’ group vector contains the class of samples: 1 = happy, 2 = sadness, corresponding to the rows of the training data matrices.\n",
    "\n",
    "Then, calculate average classification performances for both training and testing data. The correct class labels corresponding with the rows of the training and testing data matrices are in the variables ‘training_class’ and ‘testing_class’, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Train SVM classifier\n",
    "clf = svm.SVC()\n",
    "clf.fit()  \n",
    "\n",
    "#The prediction results of training data and testing data respectively\n",
    "[] = clf.predict()\n",
    "[] = clf.predict()\n",
    "\n",
    "#Calculate and Print the training accuracy and testing accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the confusion matrix through __[`sklearn.metrics.confusion_matrix()`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)__function for training data and testing data respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What is PCA? Why we use PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. \n",
    "Based on Task1, use Canonical Correlation Analysis to calculate the correlation coefficient of facial expression and audio features. Finally, use CCA to build a multi-modal emotion recognition system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use (__[`sklearn.cross_decomposition.CCA()`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html)__) function to calculate the correlation coefficient of facial expression and audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "import numpy as np\n",
    "\n",
    "#Use CCA to construct the Canonical Projective Vector (CPV)\n",
    "cca = CCA()\n",
    "cca.fit()\n",
    "\n",
    "#Construct Canonical Correlation Discriminant Features (CCDF) for training data and testing data\n",
    "[]  = cca.transform()\n",
    "[]  = cca.transform()\n",
    "\n",
    "\n",
    "# Concatenate multiple feature for training data and testing data respectively\n",
    "training_CCDF = np.concatenate()\n",
    "testing_CCDF = np.concatenate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train SVM classifiers through  'linear' kernel, print the training and testing accuracy and compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train svm classifier \n",
    "clf = svm.SVC()\n",
    "clf.fit()  \n",
    "\n",
    "#The prediction results for training data and testing data \n",
    "prediction_train = clf.predict()\n",
    "prediction_test = clf.predict()\n",
    "\n",
    "#Calculate and Print the training accuracy and testing accuracy. \n",
    "\n",
    "#Compute the confusion matrix through sklearn.metrics.confusion_matrix()function for training data and testing data respectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What is CCA? Why we use CCA here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. \n",
    "Here is a task to recognize pain through facial-expressions and ECG signals. Firstly, use ECG signals and vidoes of facial expressions to recognize the pain results. Finally, find a way (such as multiply and sum) to combine the decisions based on ECG signals and vidoes of facial expressions to build a multi-modal recognition system for better result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data: \n",
    "We load the facial expression data and ECG data(training data ('ecg_train' and 'video_train'), training class ('label_train'), testing data ('ecg_test' and 'video_test'), testing class('label_test')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata = sio.loadmat('ecg_facial_expression_video.mat')\n",
    "\n",
    "#facial expression video training and testing data\n",
    "training_data_video = \n",
    "testing_data_video = \n",
    "\n",
    "\n",
    "#ECG training and testing data, training and testing data\n",
    "training_data_ecg = \n",
    "testing_data_ecg = \n",
    "\n",
    "#training and testing class\n",
    "training_class_ecg_video = \n",
    "testing_class_ecg_video = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature classification\n",
    "Use the __[`SVM`](http://scikit-learn.org/stable/modules/svm.html)__ function to train Support Vector Machine (SVM) classifiers.\n",
    "Construct two SVM using linear kernel with C=1000 to classify facial video data and ECG data seprately. The ‘training_class’ group vector contains the class of samples: 1 = pain, 0 = no pain, corresponding to the rows of the training data matrices.\n",
    "\n",
    "Then, calculate average classification performances for both training and testing data. The correct class labels corresponding with the rows of the training and testing data matrices are in the variables ‘training_class’ and ‘testing_class’, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train svm classifier for facial expression data\n",
    "clf = svm.SVC()\n",
    "clf.fit()  \n",
    "\n",
    "#The prediction results\n",
    "prediction_train_video = clf.predict()\n",
    "prediction_video = clf.predict()\n",
    "\n",
    "#Calculate and Print the training accuracy and testing accuracy. \n",
    "\n",
    "\n",
    "# Compute the confusion matrix through sklearn.metrics.confusion_matrix()function for training data and testing data respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train svm classifier for ECG signals\n",
    "clf = svm.SVC()\n",
    "clf.fit()  \n",
    "\n",
    "#The prediction results\n",
    "prediction_train_ecg = clf.predict()\n",
    "prediction_ecg = clf.predict()\n",
    "\n",
    "#Calculate and Print the training accuracy and testing accuracy. \n",
    "\n",
    "\n",
    "# Compute the confusion matrix through sklearn.metrics.confusion_matrix()function for training data and testing data respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision fusion: \n",
    "Design a stragety to fuse the decisions based on facial expression and ECG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to combine the decisions of ECG and videos to get a better result\n",
    "\n",
    "\n",
    "#Calculate and Print the testing accuracy. \n",
    "\n",
    "\n",
    "#Compute the confusion matrix through sklearn.metrics.confusion_matrix()function for testing data respectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What is difference between decision-level fusion and feautre-level fusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: \n",
    "Use feature-level method (Task 2) on 10-fold cross-validation estimate of the emotion recognition system performance\n",
    "* Join the training/testing data matrices and the class vectors. Combine also the ‘training_data_personID’ and ‘testing_data_personID’ vectors that are needed to make the CV folds.\n",
    "* Construct the CV folds by training ten SVMs. For each SVM nine persons’ data is used as the training set (i.e. 90 samples) and one persons’ samples are kept as the test set (i.e. 10 samples) for the respective fold (i.e. each SVM has different persons’ samples excluded from the training set). Test each ten trained SVMs by using the corresponding one held-out persons’ samples and then calculate the average classification performances for each fold.\n",
    "* Calculate the mean and SD of the ten CV fold performances to produce the final CV performance estimate of the emotion recognition system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mv",
   "language": "python",
   "name": "mv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
